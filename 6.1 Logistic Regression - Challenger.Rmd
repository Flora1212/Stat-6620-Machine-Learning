

1. Perform the Logistic Regression analysis of the challenger data. Produce a report using an Rnotebook explaining the data, the analysis, and the findings.
Organize you report using the Five Steps.

# Step 1 - collecting data
On January 28, 1986, seven crew members of the United States space shuttle Challenger were killed when a rocket booster failed, causing a catastrophic disintegration. The Challenger dataset contains 4 variables. The response variable is distress_ct, and predictors are temperature, field_check_pressure and flight_num. We conduct logistic regression analysis to predict catastrophic failures.

# Step 2 - exploring and preparing the data

##Import challenger dataset into Rstudio
```{r}
launch <- read.csv("http://www.sci.csueastbay.edu/~esuess/classes/Statistics_6620/Presentations/ml10/challenger.csv")
str(launch)
```
We can see that the dataset contains 23 observations and 4 variables.

##Recode the distress_ct variable into 0 and 1, making 1 to represent at least one failure during a launch.
```{r}
launch$distress_ct = ifelse(launch$distress_ct<1,0,1)
launch$distress_ct
```

##Set up trainning and test data sets
```{r}
set.seed(123)
indx = sample(1:nrow(launch), as.integer(0.9*nrow(launch)))
indx

launch_train = launch[indx,]
launch_test = launch[-indx,]

launch_train_labels = launch[indx,1]
launch_test_labels = launch[-indx,1]   
```
We use 90% dataset as the training data and the left 10% as the testing data.

##Check if there are any missing values
```{r}
library(Amelia)
missmap(launch, main = "Missing values vs observed")
```
Use missmap() function in package Amelia, we can see that there is no missing data.

Then, we need to check for missing values and look how many unique values there are for each variable using the sapply() function which applies the function passed as argument to each column of the dataframe.

##number of missing values in each column
```{r}
sapply(launch,function(x) sum(is.na(x)))
```



##number of unique values in each column
```{r}
sapply(launch, function(x) length(unique(x)))
```
We can see that distress_ct only has two unique values 0 and 1. field_check_pressure has 3 unique values: 50, 100 and 200.

# Step 3 - training a model on the data
##fit the logistic regression model, with all predictor variables
```{r}
model <- glm(distress_ct ~.,family=binomial(link='logit'),data=launch_train)
model

summary(model)

```
We use glm() to fit the logistic regression model, and need to specify the parameter family=binomial in the glm() function. From the outputs we can see that the p-vlaue for temperature is relatively small (<0.1) and large for other preditors (>0.1), indicating that temperature has a significant effect on distress_ct, but others don't.

Then, We run anova() function to analyze the table of deviance.
```{r}
anova(model, test="Chisq")
```
The difference between the null deviance and the residual deviance shows how our model is doing against the null model (a model with only the intercept). The wider this gap, the better. Analyzing the table we can see the drop in deviance when adding each variable one at a time. 

Adding temperature significantly (p-value = 0.01478) reduces the residual deviance. A large p-value here indicates that the model without the variable explains more or less the same amount of variation.So field_check_pressure and flight_num have insignificant effects. Ultimately what we would like to see is a significant drop in deviance and the AIC.

# Step 4&5 - improving and evaluating model performance
##drop the insignificant predictors, alpha = 0.10
We only keep the significant predictor tempreture and run the logistic regression model again.
```{r}
model <- glm(distress_ct ~ temperature,family=binomial(link='logit'),data=launch_train)
model

summary(model)

anova(model, test="Chisq")
```

We can see that the AIC decreases in this model. From 23.85 to 20.55.

##check Accuracy on the improved model
```{r}
fitted.results <- predict(model,newdata=launch_test,type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)

misClasificError <- mean(fitted.results != launch_test$distress_ct)
print(paste('Accuracy',1-misClasificError))

```
Now we would like to see how the moel is doing when predicting y on the testing datasete. By setting the parameter type='response', R will output probabilities in the form of P(y=1|X). Our decision boundary will be 0.5. If P(y=1|X) > 0.5 then y = 1 otherwise y=0. Note that for some applications different thresholds could be a better option.

The output shows a prediction of accuracy of 66.7%.

##plot ROC
As a last step, we are going to plot the ROC curve and calculate the AUC (area under the curve) which are typical performance measurements for a binary classifier.

The ROC is a curve generated by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings while the AUC is the area under the ROC curve. As a rule of thumb, a model with good predictive ability should have an AUC closer to 1 (1 is ideal) than to 0.5.

Notice: Because this data set is so small, it is possible that the test data set does not contain both 0 and 1 values. If this happens the code will not run. And since the test data set is so small the ROC is not useful here but we still run the code here.
```{r}
library(ROCR)
p <- predict(model, newdata=launch_test, type="response")
pr <- prediction(p, launch_test$distress_ct)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```




